{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorboardX -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T18:56:48.395166Z",
     "iopub.status.busy": "2025-03-08T18:56:48.394850Z",
     "iopub.status.idle": "2025-03-08T18:56:48.401773Z",
     "shell.execute_reply": "2025-03-08T18:56:48.400943Z",
     "shell.execute_reply.started": "2025-03-08T18:56:48.395136Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data.distributed\n",
    "\n",
    "# from model import pyramidnet\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='cifar10 classification models')\n",
    "parser.add_argument('--lr', default=0.1, help='')\n",
    "parser.add_argument('--resume', default=None, help='')\n",
    "parser.add_argument('--batch_size', type=int, default=768, help='')\n",
    "parser.add_argument('--num_workers', type=int, default=4, help='')\n",
    "parser.add_argument(\"--gpu_devices\", type=int, nargs='+', default=None, help=\"\") \n",
    "# nargs can take more than value, as a list  --> if only one value is passed it is treated as non-iter then it may throw error TypeError: 'list' object cannot be interpreted as an integer\n",
    "\n",
    "parser.add_argument(\"--epochs\", type=int, default=None, help=\"\")\n",
    "\n",
    "parser.add_argument('--gpu', default=None, type=int, help='GPU id to use.')\n",
    "parser.add_argument('--dist-url', default='tcp://127.0.0.1:3456', type=str, help='')\n",
    "parser.add_argument('--dist-backend', default='nccl', type=str, help='')\n",
    "parser.add_argument('--rank', default=0, type=int, help='')\n",
    "parser.add_argument('--world_size', default=1, type=int, help='')\n",
    "parser.add_argument('--distributed', action='store_true', help='')\n",
    "args = parser.parse_args()\n",
    "\n",
    "gpu_devices = ','.join([str(id) for id in args.gpu_devices])\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_devices\n",
    "\n",
    "\n",
    "model = torchvision.models.resnet18(weights = 'DEFAULT')\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "\n",
    "# summary(model, (1, 3, 224, 224), device = \"cpu\")\n",
    "# model.to(device) # move to gpu\n",
    "print(\"--- Model Loaded --- \")\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    ngpus_per_node = torch.cuda.device_count()\n",
    "\n",
    "    args.world_size = ngpus_per_node * args.world_size\n",
    "    mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))\n",
    "        \n",
    "        \n",
    "def main_worker(gpu, ngpus_per_node, args):\n",
    "    args.gpu = gpu\n",
    "    ngpus_per_node = torch.cuda.device_count()    \n",
    "    print(\"Use GPU: {} for training\".format(args.gpu))\n",
    "        \n",
    "    args.rank = args.rank * ngpus_per_node + gpu    \n",
    "    dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                            world_size=args.world_size, rank=args.rank)\n",
    "\n",
    "    print('==> Making model..')\n",
    "    net = model\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    net.cuda(args.gpu)\n",
    "    args.batch_size = int(args.batch_size / ngpus_per_node)\n",
    "    args.num_workers = int(args.num_workers / ngpus_per_node)\n",
    "    net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[args.gpu])\n",
    "    num_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    print('The number of parameters of model is', num_params)\n",
    "\n",
    "    print('==> Preparing data..')\n",
    "    transforms_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "    dataset_train = CIFAR10(root='./data', train=True, download=True, \n",
    "                            transform=transforms_train)\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset_train)\n",
    "    train_loader = DataLoader(dataset_train, batch_size=args.batch_size, \n",
    "                              shuffle=(train_sampler is None), num_workers=args.num_workers, \n",
    "                              sampler=train_sampler)\n",
    "\n",
    "    # there are 10 classes so the dataset name is cifar-10\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=args.lr, \n",
    "                          momentum=0.9, weight_decay=1e-4)\n",
    "    num_epochs = args.epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        train(net, criterion, optimizer, train_loader, args.gpu)\n",
    "            \n",
    "\n",
    "def train(net, criterion, optimizer, train_loader, device):\n",
    "    net.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    epoch_start = time.time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        start = time.time()\n",
    "        \n",
    "        inputs = inputs.cuda(device)\n",
    "        targets = targets.cuda(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        acc = 100 * correct / total\n",
    "        \n",
    "        batch_time = time.time() - start\n",
    "        \n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Epoch: [{}/{}]| loss: {:.3f} | acc: {:.3f} | batch time: {:.3f}s '.format(\n",
    "                batch_idx, len(train_loader), train_loss/(batch_idx+1), acc, batch_time))\n",
    "    \n",
    "    elapse_time = time.time() - epoch_start\n",
    "    elapse_time = datetime.timedelta(seconds=elapse_time)\n",
    "    print(\"Training time per epoch {}\".format(elapse_time))\n",
    "    \n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T18:56:48.504671Z",
     "iopub.status.busy": "2025-03-08T18:56:48.504441Z",
     "iopub.status.idle": "2025-03-08T18:59:52.962173Z",
     "shell.execute_reply": "2025-03-08T18:59:52.961362Z",
     "shell.execute_reply.started": "2025-03-08T18:56:48.504652Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Loaded --- \n",
      "--- Model Loaded --- \n",
      "Use GPU: 1 for training\n",
      "--- Model Loaded --- \n",
      "Use GPU: 0 for training\n",
      "==> Making model..\n",
      "==> Making model..\n",
      "The number of parameters of model is The number of parameters of model is11181642 \n",
      "11181642==> Preparing data..\n",
      "\n",
      "==> Preparing data..\n",
      "Files already downloaded and verifiedFiles already downloaded and verified\n",
      "\n",
      "--- Model Loaded --- \n",
      "--- Model Loaded --- \n",
      "--- Model Loaded --- \n",
      "--- Model Loaded --- \n",
      "Epoch: [0/66]| loss: 2.560 | acc: 11.458 | batch time: 0.807s \n",
      "Epoch: [0/66]| loss: 2.609 | acc: 13.802 | batch time: 0.931s \n",
      "Epoch: [20/66]| loss: 2.093 | acc: 34.772 | batch time: 0.095s \n",
      "Epoch: [20/66]| loss: 2.130 | acc: 34.338 | batch time: 0.092s \n",
      "Epoch: [40/66]| loss: 1.876 | acc: 42.124 | batch time: 0.096s Epoch: [40/66]| loss: 1.819 | acc: 42.537 | batch time: 0.093s \n",
      "\n",
      "Epoch: [60/66]| loss: 1.857 | acc: 43.327 | batch time: 0.097s \n",
      "Epoch: [60/66]| loss: 1.814 | acc: 43.716 | batch time: 0.096s \n",
      "Training time per epoch 0:00:32.633945\n",
      "Training time per epoch 0:00:32.641387\n",
      "--- Model Loaded --- \n",
      "--- Model Loaded --- \n",
      "--- Model Loaded --- \n",
      "--- Model Loaded --- \n",
      "Epoch: [0/66]| loss: 2.301 | acc: 20.573 | batch time: 0.209s \n",
      "Epoch: [0/66]| loss: 2.623 | acc: 16.667 | batch time: 0.163s \n",
      "Epoch: [20/66]| loss: 2.263 | acc: 18.242 | batch time: 0.214s \n",
      "Epoch: [20/66]| loss: 2.346 | acc: 18.973 | batch time: 0.091s \n",
      "Epoch: [40/66]| loss: 2.221 | acc: 21.316 | batch time: 0.091s Epoch: [40/66]| loss: 2.255 | acc: 20.389 | batch time: 0.189s \n",
      "\n",
      "Epoch: [60/66]| loss: 2.176 | acc: 22.878 | batch time: 0.165s \n",
      "Epoch: [60/66]| loss: 2.192 | acc: 23.045 | batch time: 0.095s \n",
      "Training time per epoch 0:00:31.929263\n",
      "Training time per epoch 0:00:31.980167\n",
      "--- Model Loaded --- \n",
      "--- Model Loaded --- \n",
      "--- Model Loaded --- \n",
      "--- Model Loaded --- \n",
      "Epoch: [0/66]| loss: 2.585 | acc: 21.875 | batch time: 0.179s \n",
      "Epoch: [0/66]| loss: 2.154 | acc: 25.260 | batch time: 0.162s \n",
      "Epoch: [20/66]| loss: 2.332 | acc: 25.893 | batch time: 0.099s \n",
      "Epoch: [20/66]| loss: 2.220 | acc: 26.662 | batch time: 0.158s \n",
      "Epoch: [40/66]| loss: 2.144 | acc: 28.411 | batch time: 0.140s \n",
      "Epoch: [40/66]| loss: 2.074 | acc: 28.792 | batch time: 0.090s \n",
      "Epoch: [60/66]| loss: 2.003 | acc: 29.982 | batch time: 0.100s Epoch: [60/66]| loss: 2.043 | acc: 29.884 | batch time: 0.151s \n",
      "\n",
      "Training time per epoch 0:00:31.931349\n",
      "Training time per epoch 0:00:31.986527\n",
      "--- Model Loaded --- \n",
      "--- Model Loaded --- \n",
      "--- Model Loaded --- \n",
      "--- Model Loaded --- \n",
      "Epoch: [0/66]| loss: 1.842 | acc: 31.250 | batch time: 0.164s \n",
      "Epoch: [0/66]| loss: 1.904 | acc: 32.031 | batch time: 0.273s \n",
      "Epoch: [20/66]| loss: 1.819 | acc: 31.994 | batch time: 0.174s Epoch: [20/66]| loss: 1.850 | acc: 31.213 | batch time: 0.101s \n",
      "\n",
      "Epoch: [40/66]| loss: 1.788 | acc: 33.708 | batch time: 0.091s \n",
      "Epoch: [40/66]| loss: 1.767 | acc: 34.210 | batch time: 0.126s \n",
      "Epoch: [60/66]| loss: 1.739 | acc: 35.468 | batch time: 0.103s Epoch: [60/66]| loss: 1.742 | acc: 35.489 | batch time: 0.103s \n",
      "\n",
      "Training time per epoch 0:00:32.106406\n",
      "Training time per epoch 0:00:32.100638\n",
      "--- Model Loaded --- \n",
      "--- Model Loaded --- \n",
      "--- Model Loaded --- \n",
      "--- Model Loaded --- \n",
      "Epoch: [0/66]| loss: 1.692 | acc: 35.938 | batch time: 0.417s Epoch: [0/66]| loss: 1.672 | acc: 39.323 | batch time: 0.156s \n",
      "\n",
      "Epoch: [20/66]| loss: 1.708 | acc: 37.140 | batch time: 0.102s \n",
      "Epoch: [20/66]| loss: 1.675 | acc: 38.777 | batch time: 0.108s \n",
      "Epoch: [40/66]| loss: 1.636 | acc: 39.685 | batch time: 0.104s \n",
      "Epoch: [40/66]| loss: 1.665 | acc: 38.523 | batch time: 0.102s \n",
      "Epoch: [60/66]| loss: 1.611 | acc: 40.450 | batch time: 0.099s \n",
      "Epoch: [60/66]| loss: 1.625 | acc: 40.027 | batch time: 0.115s \n",
      "Training time per epoch 0:00:32.473986\n",
      "Training time per epoch 0:00:32.477097\n",
      "[rank0]:[W308 18:59:50.301727817 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "# cd dist_parallel\n",
    "# For Kaggle T4x2 So we pass two ids\n",
    "!python train.py --gpu_device 0 1  --batch_size 768 --epochs 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
